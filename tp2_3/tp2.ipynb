{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "This series of exercises uses the mdp tool box developped at INRAE\n",
    "    https://pymdptoolbox.readthedocs.io/en/latest/index.html\n",
    "        \n",
    "Install it (if not already installed) and import the toolbox and the examples\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example, mdptoolbox.util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Example: Forest Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The considered problem is to manage a forest stand with first the objective to maintain an old forest for wildlife and second to make money selling cutwood. \n",
    "\n",
    "The forest stand is managed by two possible actions: Wait (action 0) or Cut (action 1). An action is decided and applied each time period  at the beginning of the period.\n",
    "\n",
    "Three states are defined, corresponding to 3 age-class of trees: age-class 0-20years (state 1), 21-40 years (state 2), more than 40 years (state 3).  The state 3 correspond to the oldest age-class. \n",
    "\n",
    "At the end of a period t, if the state was s at t and action Wait is choosen, the state at the next time period will be min(s+ 1,3) if no fire occured (the forest grows). But there is a probability p that a fire burns the forest after the application of the action, leaving the stand at the youngest age-class (state 1). \n",
    "\n",
    "Let p = 0.1 be the probability of wildfire occurence during a time period. The problem is how to manage this stand in a long term vision to maximize the γ-discounted reward with γ= 0.95\n",
    "\n",
    "\n",
    "mdptoolbox.example.forest() provides a modelisation of this problem in the MDP framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Q1. create and check  a mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating a forest management problem with 3 states, a direct cut reward equal to 1 but for the last state (where it is equal to 2)\n",
    "# a preservation reward equal to 4, and a probability of fire equal to 0.1\n",
    "# (for more details,  look  at the \"example\" module)\n",
    "    \n",
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "\n",
    "# the following call checks that the MDP is correctly defined \n",
    "#(for more details,  look  at the \"util\" module : https://pymdptoolbox.readthedocs.io/en/latest/api/util.html)\n",
    "\n",
    "mdptoolbox.util.check(P, R) # Nothing should happen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Observe the transition matrix (P) and the reward matrix (R)   \n",
    " \n",
    " In particular check  probability to be in state 3 at time t+1 (s’ = 3), when being in state 3 (s = 3) and choosing action Wait (a = 0) at time t, is P(3,3,1) = 1 - p = 0.9 and the associated reward is R(3,1) = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.9, 0. ],\n",
       "        [0.1, 0. , 0.9],\n",
       "        [0.1, 0. , 0.9]],\n",
       "\n",
       "       [[1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Policy iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the policy iteration algorithm and look at  the policy computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tNumber of different actions\n",
      "    1\t\t  1\n",
      "    2\t\t  0\n",
      "Iterating stopped, unchanging policy found.\n"
     ]
    }
   ],
   "source": [
    "# Apply the policy iteration algorithm\n",
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.setVerbose()\n",
    "pi.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the policy associated to pi\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26.244000000000014, 29.484000000000016, 33.484000000000016)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display  its value \n",
    "\n",
    "pi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the minimal for probabilty of fire shall  that implies that it is better to cut   the forest  than wait in state 2 (age 21-40)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tNumber of different actions\n",
      "    1\t\t  1\n",
      "    2\t\t  0\n",
      "Iterating stopped, unchanging policy found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.setVerbose()\n",
    "pi.run()\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Value iteration\n",
    "\n",
    "Look at https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html#mdptoolbox.mdp.ValueIteration\n",
    "    \n",
    "And apply the Value iteration to the same problem (once with p=0.1, once with p=0.79) \n",
    "\n",
    "You can modify the  epsilon threshold and observe the number of iteration needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration\t\tV-variation\n",
      "    1\t\t  4.0\n",
      "    2\t\t  2.5029\n",
      "    3\t\t  0.9122714100000007\n",
      "    4\t\t  0.0489874447889993\n",
      "    5\t\t  0.01884470075378175\n",
      "    6\t\t  0.02190519408847269\n",
      "    7\t\t  0.019937784643719425\n",
      "    8\t\t  0.017800582030833567\n",
      "    9\t\t  0.01586472175726783\n",
      "    10\t\t  0.01413712536759526\n",
      "    11\t\t  0.012597471054032638\n",
      "    12\t\t  0.01122548314916827\n",
      "    13\t\t  0.010002916915990312\n",
      "    14\t\t  0.008913500145983022\n",
      "    15\t\t  0.007942731648864054\n",
      "    16\t\t  0.007077689460533776\n",
      "    17\t\t  0.006306858938941673\n",
      "    18\t\t  0.005619979500028904\n",
      "    19\t\t  0.005007908038937359\n",
      "    20\t\t  0.004462497225517836\n",
      "    21\t\t  0.003976487054679012\n",
      "    22\t\t  0.0035434082077507867\n",
      "    23\t\t  0.0031574959390354707\n",
      "    24\t\t  0.0028136133407379305\n",
      "    25\t\t  0.0025071829652532074\n",
      "    26\t\t  0.0022341258943576747\n",
      "    27\t\t  0.0019908074444501267\n",
      "    28\t\t  0.0017739887850112268\n",
      "    29\t\t  0.001580783826231169\n",
      "    30\t\t  0.0014086208021169\n",
      "    31\t\t  0.0012552080374490515\n",
      "    32\t\t  0.0011185034431555607\n",
      "    33\t\t  0.0009966873339166682\n",
      "Iterating stopped, epsilon-optimal policy found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25.5833879767579, 28.830654635546928, 32.83065463554693)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R = mdptoolbox.example.forest(3,4,2,0.1)\n",
    "pi = mdptoolbox.mdp.ValueIterationGS(P, R, 0.9, 0.01)\n",
    "pi.setVerbose()\n",
    "pi.run()\n",
    "pi.policy\n",
    "pi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.   Setting the parameters\n",
    "\n",
    "\n",
    "\n",
    "You can also have a look at the cpu time neeed for the last run of the algorithm ; \n",
    "\n",
    "Considering a bigger problem (ex: 100 states),\n",
    "\n",
    "P, R = mdptoolbox.example.forest(100,4,2,0.1)\n",
    "\n",
    "which is the best policy (when to begin the cut ?)\n",
    "\n",
    "which is the average time (on 10 runs) for each algo ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24481964111328125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R = mdptoolbox.example.forest(100,4,2,0.1)\n",
    "#pi = mdptoolbox.mdp.ValueIterationGS(P, R, 0.9)\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.run()\n",
    "#pi.policy\n",
    "pi.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01185038113834882\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for i in range(1,100):\n",
    "    P, R = mdptoolbox.example.forest(100,4,2,0.1)\n",
    "    pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "    pi.run()\n",
    "    t.append(pi.time)\n",
    "\n",
    "t_mean = np.mean(t)\n",
    "print(t_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an instance of  ValueIterationGS  or of PolicyIteration, you can play with the parameters of the solving\n",
    "\n",
    "\n",
    "class mdptoolbox.mdp.ValueIterationGS(transitions, reward, discount, epsilon=0.01, max_iter=10, initial_value=0, skip_check=False)\n",
    "\n",
    "class mdptoolbox.mdp.PolicyIteration(transitions, reward, discount, policy0=None, max_iter=1000, eval_type=0, skip_check=False)\n",
    "\n",
    "(look again at https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html)\n",
    "\n",
    "\n",
    "Which is the best parametrization to have the quickest solving with   epsilon=0.01 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
